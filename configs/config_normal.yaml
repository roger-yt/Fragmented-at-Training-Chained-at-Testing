# experiment settings
hydra:
  run:
    dir: .
  output_subdir: null      # ⇦ stops “.hydra/…” creation
  job:
    chdir: false           # ⇦ keep CWD unchanged
  
graph:
  type: 0
  len: 5
  width: 2
  merge_pos: 0

tokenizer:
  model_name: gpt2

model:
  n_layers: 3
  n_heads: 3
  hidden_size: 720
  vocab_size: 52
  name: gpt2

data:
  max_child_chain_len: 3
  max_examples: 5
  num_icl_train: 5000
  num_icl_valid: 100
  num_mk_train: 20000
  num_mk_valid: 100
  context_lower: 1
  context_upper: 7
  context_div: 7
  env_val_num_low: 10
  chain_val_num: 50
  leak_prob_node: 0.0
  leak_prob_val: 0.0
  addlen: 5
  nearlen: 100
  tl_low: 4

trainer:
  _target_: transformers.TrainingArguments
  eval_strategy: epoch
  num_train_epochs: 14
  save_steps: 8_000
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 1
  learning_rate: 5e-5
  report_to: none

paths:
  data_dir: none

modes:
  train: true
  test: true
  probe: false 
  plot: false

test:
  epoch: -1

probe:
  mean_num: 10

draw:
  type: standard
  mode: main
  parent_dir: none
  name: acc_map_1
  model_size: normal